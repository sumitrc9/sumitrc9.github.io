<!DOCTYPE html>
<html>
<head>
	<title>Ethics of Autonomous Vehicles: Moral Decision Making</title>
</head>
<body>
	<h1>Moral Decision Making</h1>

	<p>

	For the past 135 years, driving has always been done by a human operator. By handing over driving, a task primarily performed by humans, to a set of algorithms, the processes which control self-driving cars will need to be able to meet the ethical standards to which people are held to. Nevertheless, the subjectivity of morality makes this difficult as a lot of gray area exists between what is right or wrong. Individual moral values are likely to vary based upon upbringing and other external factors and cannot be strictly defined. Thus, in order to design a vehicle that is morally sound, it becomes necessary to identify the ethical standards upheld by the vast majority of society. 

	</p>

	<p>
	
	Classical ethics can serve as a starting point for evaluating the moral standards of autonomous vehicles. Two of the major lenses of classical ethics are consequentialism and deontology. 

	</p>

	<p>
	From a consequentialist's perspective, an act's morality is dependent only on the results of the act, and the goodness of the act is determined by its consequence [1]. With respect to autonomous vehicles, the most common consequentialist approach is utilitarianism. This school of thought emphasizes that a moral decision is one that maximizes some form of utility. However, the decisions that a self-driving car can make can vastly differ based on what metric is used to measure utility [2]. One measure of utility could be human life. If a self-driving car was programmatically designed in this fashion, it would always prioritize a decision that saves that most people. For example, this type of autonomous vehicle would choose to crash into a building and kill the river over hitting five people in the middle of the road. Many people think that this pure consequentialist approach is not ethically sound as the vehicle should not be able to declare a single life to be forfeit. Another utilitarian approach to self-driving cars is having utility be measured through economic costs. Presented with a similar scenario, the car would choose to hit the five people instead because the damage to the building and the vehicle would lead to higher economic costs. Once again, most people thought that this approach also does not properly capture the ethical standards held by individuals. This goes to show that a strict utilitarian approach may not be ideal for autonomous vehicles.
	</p>

	<p>
	Rather than worrying about consequences, the deontological perspective of ethics is concerned with individual actions and an obligation to behave morally [3]. This approach to ethics requires a predefined set of rules which the algorithms of a self-driving car will need to abide by. However, a problem with applying deontological ethics to autonomous vehicles is that complex human values do not easily translate into computer code [2]. For example, one possible ethical rule would be to preserve the lives of older people over younger people. Having such a rule would mean that the vehicle would need to be able to  identify the age of all people in its near vicinity. A similar issue arises if the car tries to distinguish between rich and poor people. This introduces an additional level of complexity into the autonomous vehicle and still does not ethically resonate with most people. A fixed set of rules may not always help the car in making moral decisions.


	</p>

	<p>


	Neither the results-based approach nor the duty-based approach provide a definite solution to moral decisions in autonomous vehicles. This is understandable as most humans do not abide by one ethical perspective either. Moral values can vary based on upbringing and individuals can differ in their ethical preferences. 

	</p>

	<p>


	In a survey called "The Moral Machine Experiment" [4], researchers surveyed three distinct groups of people to identify how moral values differ based upon location. The participants were shown a multitude of no-win scenarios that an autonomous vehicle may hypothetically face and were asked to make the "moral" decision. Based of the results of the study, the global preferences for moral behavior lean towards sparing humans over animals, saving more lives, protecting lawful over unlawful citizens, and sparing younger lives. Nevertheless, the study also showed that ethical values varied significantly based on culture. For example, people from the Eastern cluster (i.e. Asian countries) were more likely to save older and more respected people than other cultural groups. The Southern cluster (i.e. places with "French" ideals) had a stronger preference towards saving women and physically fit individuals. This experiment served as one of the first global discussions regarding how a moral machine should behave. By understanding the results of this survey, the designers of autonomous vehicles will be one step closer towards designing a self-driving car that will meet the ethical standards of cultures all around the world.


	</p>


	<p>
	
	Oftentimes, arguments about the moral decision making abilities of an autonomous vehicle have been scrutinized as being irrelevant since many of the scenarios that are used to evaluate the morality of the vehicle will likely never occur in the real world. It can be said that the overall improvement to safety will overshadow the debate regarding how vehicles need to behave in hypothetical scenarios. However, as AI begins to automate an increasing number of tasks that require higher cognitive abilities, the ethics of moral machines need to be addressed.

	</p>
	<p>

	There is no correct answer to the moral question. Moral machines do not need to be ideal ethical beings to function satisfactorily in responding to morally significant situations. Regardless, building these machines requires a comprehensive model for how humans arrive at satisfactory moral judgments [5]. Rather than being morally perfect, the algorithms which drive self-driving cars will be expected to be held to the same moral standards as human beings. Understanding morality from various classical and cultural perspectives provides more insight into the complexities of having a machine meet these moral standards.


	</p>

	<p>
	References:

	[1] “Ethics - Introduction to ethics: Consequentialism,” BBC, 2014. [Online]. Available: http://www.bbc.co.uk/ethics/introduction/consequentialism_1.shtml. [Accessed: 27-July-2020].

	[2] N. J. Goodall, “Machine Ethics and Automated Vehicles,” SpringerLink, Jun-2014. [Online]. Available: https://link.springer.com/chapter/10.1007/978-3-319-05990-7_9. [Accessed: 27-Jul-2020].

	[3] “Ethics - Introduction to ethics: Duty-based ethics,” BBC, 2014. [Online]. Available: http://www.bbc.co.uk/ethics/introduction/duty_1.shtml. [Accessed: 27-July-2020].

	[4] E. Awad, S. Dsouza, R. Kim, J. Schulz, J. Henrich, A. Shariff, J.-F. Bonnefon, and I. Rahwan, “The Moral Machine experiment,” Nature News, 24-Oct-2018. [Online]. Available: https://www.nature.com/articles/s41586-018-0637-6. [Accessed: 27-July-2020].

	[5] W. Wallach, “Robot minds and human ethics: the need for a comprehensive model of moral decision making,” Ethics and Information Technology, 07-Jul-2010. [Online]. Available: https://link.springer.com/article/10.1007/s10676-010-9232-8. [Accessed: 27-July-2020].

	</p>

</body>
</html>